layout: post
title:  "XAI 소개"
authors: [김민섭]
tags: ["XAI", "BLACKBOX"]
image: assets/images/KakaoTalk_20230327_181749421.jpg
featured: true

## xai란?
머신러닝과 ai가 현재 뜨거운 감자인 만큼 현재 우리 삶에도 많이 밀접해 있다고 말할 수 있는데,
이번 글에서는 현재 ai의 한 문제점과 그 문제점에 대한 해결 방안의 탐색에 대해서 다룰 예정입니다.
인공지능이 어떤 근거로 상황 판단을 하는 지에 대해서는 한계가 있고,
ai의 사고방식을 그대로 알아내는 것 자체가 사실상 불가능하다는 것을 인공지능에서는 인공지능의 ‘블랙박스(Black Box) 문제점‘이라고 하는데,
인공지능이 어떻게 사고하고 판단하는지는 사실상 검은 상자에 가려져 있다는 의미에서 붙여진 이름입니다.
최근 흔히 사용되는 인공지능의 대다수가 딥러닝 기법을 사용하고 있는데,
딥러닝 방식의 가장 큰 특징은 입력 값과 출력 값만을 제공해 준다면 컴퓨터가 스스로 학습을 해낼 수 있다는 점입니다.
다시 말해 딥러닝 모델이 이미 있다면 인간이 관여할 수 있는 일은 입력 값과 출력 값의 제공에 대한 것 뿐입니다.
그러므로 ai가 어떤 근거를 가지고 의사결정하는지는 알 수 없고, 직접적으로 판단하는 장면을 관찰하지 못합니다.
컴퓨터는 이미지를 텍스트를 포함한 모든 데이터를 ‘숫자’ 형식으로 읽고 진행하니,
이것에 대해 무언가를 연구하면 인공지능의 연산을 알 수 있지 않을까하지만,
이는 단순 확률 값에 불과하므로 숫자 조합과 연산에서 특별한 의미를 도출하는 것은 사실상 불가능에 가깝습니다.

![image](../assets/images/xxxx/xxxx)

예를 들면 마약 탐지견이 마약을 잘 찾아내는 것은 알고 있지만, 대체 어떤 냄새를 맡고 정확히 찾아내는 지는 모르듯,
인공지능의 성능에 대해서는 의심을 하지 않지만 인공지능의 결정 과정을 잘 모르는 것이 인공 신경망의 특징입니다.
예를 들어 은행에서 신용 대출 승인 여부를 판단하는 ai를 도입했을 때 고객이 대출 신청을 했지만 이가 거절된다면
고객은 은행에 사유나 근거를 요구할 수 있는데, 인공지능의 결정 과정을 모른다면 은행에서는 그 이유를 ‘ai에 기반하여 거절되었습니다’라는 답변밖에 할 수 없습니다.
복잡한 연산 과정을 통해 ai가 문제를 해결해 나간다는 것은 알겠으나,
사람이 이해하기 쉬운 직관적인 내용으로 ai의 의사결정 과정을 알 수 있다면 인공지능 기술을 도입하고 있는 기업에서는 그 판단 방식을 이용해
다른 분야에도 적용 가능할 것이므로,  머신러닝과 ai를 사용하는 기업에서 ai에 대한 의존도가 점차 늘어감에 따라,
ai 모델이 어떻게 의사결정을 내리는가를 알고 이해할 필요성이 점차 커지고 있습니다.
xai는 설명 가능한 인공지능(explainable artificial intelligence)의 약자로,
인공지능의 의사결정의 원인과 결과를 사람이 이해 가능한 형태로 설명하는 방법론과 분야를 의미합니다.
xai는 인공지능의 불확실성을 해소하고 신뢰성을 높이는 역할을 합니다.

![image](../assets/images/xxxx/xxxx)

데이터 사이언티스트들이 수년간 사용해 온 일련의 모범 사례들을 통해 모델의 훈련 방식을 파악하도록 하는데,
모델의 훈련 방식과 훈련에 쓰인 데이터 종류를 알 수 있다면, 해당 모델을 언제 적절히 쓸 수 있는 지를 쉽게 파악할 수 있고,
노출될 수 있는 편향 등도 알 수 있는 등의 예시를 이해한다면 xai의 도입 과정에 대한 이해가 더 쉬울 것입니다.
xai의 표준화 문제를 다루는 핵심사항들은 다음과 같습니다
	모델을 설명하는 대상은 누구인가?
	설명은 얼마나 정확하고 얼마나 정밀해야 하는가?
	설명하는 대상은 전체적인가? 특정한 의사결정인가?

대부분 모델을 설명할 때는 ‘프록시 모델링’, ‘설명 가능성을 위한 설계’로 두가지 범주로 나뉘는데,
프록시 모델링이라는 기술을 사용한다면 간단하고 이해하기 쉬운 모델을 통해 세부적인 ai모델을 상당히 정확하게 설명할 수 있고,
이러한 설명을 통해 모델을 전체적으로 이해할 수 는 있지만, 프록시 모델링은 항상 근사치 이므로 아무리 잘 적용된다 하더라도
실제로 내려진 의사결정은 프록시 모델과는 다를 수 있습니다. ‘설명 가능성을 위한 설계’는 ai 설계와 훈련을 제한해,
단순한 방식을 불러일으키는 구성요소들로 네트워크 전체를 이루게해서 설명하기에 훨씬 쉬운 방식으로 구성되지만, 구현이 생각보다 쉽지 않습니다.
툴 상자에서 쓸 수 있는 구성요소와 구조를 제한했기 때문에 효율성과 정확성이 낮아지기 때문입니다.
이번 글에서는 다루지 않을 것이지만 xai는 전체적인 대상을 설명하는 것보다 개별 의사결정을 가장 잘 설명합니다.

아직 연구가 필요한 분야이고 제가 아직 지식이 충분치 않아 ‘이런 것이 있다’ 정도에 그치는 내용이라 사과의 말씀을 드립니다.

기존 학습 모델을 수정해 학습 모델 내 의미있는 노드의 속성을 연결 하여 학습한 후 분류 결과에 대한 근거를 제공하거나
데이터 처리 과정을 확인할 수 있도록 설명을 위한 알고리즘을 더하거나 또는 판단 결과와 같이 데이터를 처리하는 과정까지 보여주는 새로운 설명 모델을 개발하거나,
타 학습 모델과 비교하여 예측 결과에 대한 근거를 제시하거나 결정 과정을 통해 설명 가능한 모델을 생성하는 등 xai에는 다양한 접근 방식이 있는데,
xai는 인공지능의 학습 과정에서 추출한 패턴 같은 것을 사용자에게 제공함으로써 인공지능의 블랙박스 문제를 해소하고 앞서 말했듯이
인공지능의 적용 분야를 확대한다는 점에서 의의가 있습니다. 하지만 ai에 비해 xai의 설명성을 평가하는 표준 방법이 없다는 점이 단점으로 꼽히므로,
xai를 실제에 적용하기 위해서는 설명 속성을 비교할 근거나 평가 기준을 마련하고 이를 검정하는 노력이 필요할 것입니다.

## 참고
http://www.incodom.kr/XAI%28explaniable_AI%29
https://blogs.nvidia.co.kr/2021/07/27/what-is-explainable-ai/
https://brunch.co.kr/@8d1b089f514b4d5/24
